{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pkl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import cropImage\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to create predictions for the challenge\n",
    "\n",
    "It uses a pickled testset and an already trained model to make predictions.\n",
    "The predictions are then packaged for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSetPath = 'example_testset.pkl'\n",
    "modelPath = 'latest_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CroppedOutImageDataSet(Dataset):\n",
    "    def __init__(self, pathToPkl):\n",
    "      self.pklPath = pathToPkl\n",
    "      with open(testSetPath,'rb') as tsp:\n",
    "            testSet = pkl.load(tsp)\n",
    "      self.images = testSet['images']\n",
    "      self.crop_sizes = testSet['crop_sizes']\n",
    "      self.crop_centers = testSet['crop_centers'] \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        image_data = np.array(self.images[id], dtype=np.float32)\n",
    "        height, width = image_data.shape\n",
    "\n",
    "        # setting some random crops\n",
    "        top = self.crop_centers[id][0] - int(self.crop_sizes[id][0]/2)\n",
    "        bottom = self.crop_centers[id][0] + int(self.crop_sizes[id][0]/2)\n",
    "        left = self.crop_centers[id][1] - int(self.crop_sizes[id][1]/2)\n",
    "        right = self.crop_centers[id][1] + int(self.crop_sizes[id][1]/2)\n",
    "\n",
    "        cropped_image, crop_array, target_array = cropImage(image_data, top, bottom, left, right)\n",
    "\n",
    "        temp = np.where(np.array(crop_array, dtype=bool), np.NaN, image_data).flatten()\n",
    "        temp = np.array([x for x in temp if not np.isnan(x)])\n",
    "        mean = temp.mean()\n",
    "        std = temp.std()\n",
    "        image_data[:] -= mean\n",
    "        image_data[:] /= std\n",
    "\n",
    "        cropped_image, crop_array, target_array = cropImage(image_data, top, bottom, left, right)\n",
    "\n",
    "        # constructing the image we want to input to the model\n",
    "        inputs = np.zeros(shape = (*cropped_image.shape, 4), dtype=cropped_image.dtype)\n",
    "        inputs[..., 0] = cropped_image\n",
    "        inputs[..., 1] = crop_array\n",
    "        inputs[..., 2] = np.tile(np.linspace(start=-1, stop=1, num=width),reps=[height,1])\n",
    "        inputs[..., 3] = np.tile(np.expand_dims(np.linspace(start=-1, stop=1, num=height),1),reps=[1,width])\n",
    "\n",
    "        return TF.to_tensor(inputs), TF.to_tensor(target_array), id, mean, std, (top, bottom, left, right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "challengeSet = CroppedOutImageDataSet(testSetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for i in range(len(challengeSet)):\n",
    "    inputs, targets, ids, mean, std, crops = challengeSet.__getitem__(i)\n",
    "\n",
    "    cropped_image = inputs[0].numpy()\n",
    "    inputs = torch.unsqueeze(inputs,0).to('cuda')\n",
    "    prediction = np.squeeze(model(inputs).cpu().detach().numpy())\n",
    "    top, bottom, left, right = crops\n",
    "    cropped_prediction = prediction[top:bottom+1,left:right+1]\n",
    "    cropped_prediction *= std\n",
    "    cropped_prediction += mean\n",
    "    cropped_prediction = np.array(cropped_prediction, dtype=np.uint8)\n",
    "    predictions.append(cropped_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_out = open('pred_exampels.pkl','wb')\n",
    "pkl.dump(predictions, pkl_out)\n",
    "pkl_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "900.6816480568282"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "from scoring import scoring\n",
    "scoring('pred_exampels.pkl','example_targets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}